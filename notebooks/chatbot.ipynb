{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from llama_index.core import SimpleDirectoryReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GEMINI_KEY=os.getenv('GEMINI_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader=SimpleDirectoryReader(input_dir='../Data/interview-question-data-science--master')\n",
    "docs=reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P a g e  5 | 16 \n",
      " \n",
      "In particular, the EmpGAN contains the empathetic generator and two interactive inverse discriminators. \n",
      "The empathetic generator is composed of three components: (i) A semantic understanding module based \n",
      "on Seq2Seq(sequence to sequence) neural networks that maintain the multi-turn semantic context. (ii) A \n",
      "multi-resolution emotion perception model captures the fine and coarse-grained emotion factors of each \n",
      "dialogue turn to build the emotional framework. (iii) An empathetic response decoder combines semantic \n",
      "and emotional context to produce appropriate responses in terms of both meaning and emotion. The two \n",
      "interactive inverse discriminator additionally incorporate the user feedback and corresponding emotional \n",
      "feedback as inverse supervised signal to induce the generator to produce a more empathetic response. \n",
      " \n",
      "Q3. G-TAD: Sub-Graph Localization for Temporal Action Detection  \n",
      "Answer: \n",
      "Video understanding has gained much attention from both academia and industry over recent years, given \n",
      "the rapid growth of videos published in online platforms. Tempora l action det ection is one of  exciting \n",
      "but challenging tasks in this  area. It involves detecting  start and the end frames of action instances, as \n",
      "well as predicting their class label. This is onerous, especially in long untrimmed videos. \n",
      " \n",
      "Video context is an important cue to detect actions effectively. Here, we refer to mean as frames that are \n",
      "outside the target action but carry valuable indicative information of it. Using video context to infer \n",
      "potential actions is natural for human beings. Empirical evidence shows that human can reliably predict \n",
      "or guess the occurrence of the specific type of work by only looking at short video snippets wh ere the \n",
      "action does not happen . Therefore, incorporating context into tempora l action detection has become  \n",
      "important strategy to boost detection accuracy in the recent literature. Researchers have proposed various \n",
      "ways to take advantage  of the video context, such as extending temporal action bou ndaries by the pre-\n"
     ]
    }
   ],
   "source": [
    "print(docs[4].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
